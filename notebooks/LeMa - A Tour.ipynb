{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Learning Machines (LeMa)! We created this platform to democratize the development of large models for the open source world. We strongly believe that together, as a community, we can push the boundaries of AI.\n",
    "\n",
    "This tutorial will give you a brief overview of LeMa's core functionality. We'll cover:\n",
    "\n",
    "1. Training a model\n",
    "1. Model Inference\n",
    "1. Evaluating a model against common benchmarks\n",
    "1. Launching jobs\n",
    "1. Customizing datasets and clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "## LeMa Installation\n",
    "First, let's install LeMa. You can find detailed instructions [here](https://github.com/openlema/lema/blob/main/README.md), but it should be as simple as:\n",
    "\n",
    "```bash\n",
    "pip install -e \".[dev,train]\"\n",
    "```\n",
    "\n",
    "## Creating our working directory\n",
    "For our experiments, we'll use the following folder to save the model, training artifacts, and our working configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tutorial_dir = \"tour_tutorial\"\n",
    "\n",
    "Path(tutorial_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeMa supports training both custom and out-of-the-box models. Want to try out a model on HuggingFace? We can do that. Want to train your own custom Pytorch model? We've got you covered there too.\n",
    "\n",
    "## A quick demo\n",
    "\n",
    "Let's try training a pre-existing model on HuggingFace. We'll use GPT2 as it's small and trains quickly.\n",
    "\n",
    "LeMa uses [training configuration files](https://learning-machines.ai/docs/latest/lema.core.types.html#lema.core.types.configs.TrainingConfig) to specify training parameters. We've already created a training config for GPT2--let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"gpt2\" # 124M params\n",
    "  model_max_length: 128\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  load_pretrained_weights: False\n",
    "  trust_remote_code: True\n",
    "  model_kwargs:\n",
    "    disable_dropout: True\n",
    "\n",
    "data:\n",
    "  train:\n",
    "    datasets:\n",
    "      - dataset_name: \"HuggingFaceFW/fineweb-edu\"\n",
    "        subset: \"sample-10BT\"\n",
    "        split: \"train\"\n",
    "    stream: True\n",
    "    pack: True\n",
    "    target_col: \"text\"\n",
    "\n",
    "training:\n",
    "  trainer_type: TRL_SFT\n",
    "  per_device_train_batch_size: 2\n",
    "  max_steps: 10\n",
    "\n",
    "  enable_gradient_checkpointing: False\n",
    "  gradient_checkpointing_kwargs:\n",
    "    use_reentrant: False\n",
    "\n",
    "  learning_rate: 6.0e-04\n",
    "  lr_scheduler_type: \"cosine_with_min_lr\"\n",
    "  lr_scheduler_kwargs:\n",
    "    min_lr_rate: 0.1\n",
    "  warmup_steps: 715\n",
    "  adam_beta1: 0.9\n",
    "  adam_beta2: 0.95\n",
    "  weight_decay: 0.1\n",
    "\n",
    "  run_name: \"gpt2_pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.core.types import TrainingConfig\n",
    "from lema.train import train\n",
    "\n",
    "config = TrainingConfig.from_yaml(str(Path(tutorial_dir) / \"train.yaml\"))\n",
    "config.training.output_dir = str(Path(tutorial_dir) / \"output\")\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've trained your first model using LeMa!\n",
    "\n",
    "You can also train your own custom Pytorch model. We cover that in depth in our [Finetuning Tutorial](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Finetuning%20Tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "Now that you've trained a model, let's run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/train_inference_config.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"tour_tutorial/output\"\n",
    "  trust_remote_code: true\n",
    "  torch_dtype_str: \"half\"\n",
    "  device_map: \"auto\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.core.types import InferenceConfig\n",
    "from lema.infer import infer\n",
    "\n",
    "config = InferenceConfig.from_yaml(\n",
    "    str(Path(tutorial_dir) / \"train_inference_config.yaml\")\n",
    ")\n",
    "\n",
    "input_text = (\n",
    "    \"Remember that we didn't train for long, \" \"so the results might not be great.\"\n",
    ")\n",
    "\n",
    "results = infer(config.model, config.generation, [[input_text]])\n",
    "\n",
    "print(results[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run inference using the pretrained model by slightly tweaking our config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_config = InferenceConfig.from_yaml(\n",
    "    str(Path(tutorial_dir) / \"train_inference_config.yaml\")\n",
    ")\n",
    "pretrained_config.model.model_name = \"gpt2\"\n",
    "\n",
    "input_text = \"Input for the pretrained model: What is your name? \"\n",
    "\n",
    "results = infer(pretrained_config.model, pretrained_config.generation, [[input_text]])\n",
    "\n",
    "print(results[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model against common benchmarks\n",
    "\n",
    "You can use LeMa to evaluate pretrained and tuned models against standard benchmarks. For example, let's evaluate the pretrained version of our GPT2 model against `Hellaswag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/eval.yaml\n",
    "\n",
    "model:\n",
    "  model_name: \"gpt2\"\n",
    "  trust_remote_code: True\n",
    "\n",
    "data:\n",
    "  datasets:\n",
    "    - dataset_name: \"hellaswag\"\n",
    "\n",
    "generation:\n",
    "  batch_size: 0  # This will let LM HARNESS decide.\n",
    "\n",
    "evaluation_framework: LM_HARNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lema.core.types import EvaluationConfig\n",
    "from lema.evaluate import evaluate\n",
    "\n",
    "eval_config = EvaluationConfig.from_yaml(str(Path(tutorial_dir) / \"eval.yaml\"))\n",
    "# Uncomment the following line to run evals against the V1 HuggingFace Leaderboard.\n",
    "# This may take a while.\n",
    "# eval_config.data.datasets[0].dataset_name = \"huggingface_leaderboard_v1\"\n",
    "\n",
    "evaluate(eval_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Jobs\n",
    "\n",
    "Often times you'll need to run various tasks (training, evaluation, etc) on remote hardware that's better suited for the task. LeMa can handle this for you by launching jobs on various compute clusters. For more information about running jobs, see our [Running Jobs Remotely tutorial](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Running%20Jobs%20Remotely.ipynb). For running jobs on custom clusters, see our [Launching Jobs on Custom Clusters tutorial](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
    "\n",
    "\n",
    "Today, LeMa supports running jobs on several cloud provider platforms.\n",
    "\n",
    "For the latest list, we can run the `which_cloud` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $tutorial_dir/job.yaml\n",
    "\n",
    "name: hello-world\n",
    "resources:\n",
    "  cloud: local\n",
    "\n",
    "# Upload a working directory to the remote.\n",
    "working_dir: .\n",
    "\n",
    "envs:\n",
    "  TEST_ENV_VARIABLE: '\"Hello, World!\"'\n",
    "  LEMA_LOGGING_DIR: \"tour_tutorial/logs\"\n",
    "\n",
    "\n",
    "run: |\n",
    "  set -e  # Exit if any command failed.\n",
    "\n",
    "  echo \"$TEST_ENV_VARIABLE\"\n",
    "  lema-train -c tour_tutorial/train.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lema.launcher as launcher\n",
    "\n",
    "print(\"Supported Clouds in LeMa:\")\n",
    "for cloud in launcher.which_clouds():\n",
    "    print(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple \"Hello World\" job locally to demonstrate how to use the LeMa job launcher. This job will echo `Hello World`, then run the same GPT2 training job executed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "job_config = launcher.JobConfig.from_yaml(str(Path(tutorial_dir) / \"job.yaml\"))\n",
    "cluster, job = launcher.up(job_config, cluster_name=None)\n",
    "\n",
    "while job and not job.done:\n",
    "    print(\"Job is running...\")\n",
    "    time.sleep(5)\n",
    "    job = cluster.get_job(job.id)\n",
    "print(\"Job is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job created logs under `/tour_tutorial/logs`. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = Path(tutorial_dir) / \"logs\"\n",
    "for log_file in logs_dir.iterdir():\n",
    "    print(f\"Log file: {log_file}\")\n",
    "    with open(log_file, \"r\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing datasets and clusters\n",
    "\n",
    "LeMa offers rich customization that allows users to build custom solutions on top of our existing building blocks. Several of LeMa's primary resources (Datasets, Clouds, etc) leverage the LeMa Registry when invoked.\n",
    "\n",
    "This registry allows users to build custom classes that function as drop-in replacements for core functionality.\n",
    "\n",
    "For more details on registering custom datasets, see the [demo here](https://github.com/openlema/lema/blob/main/USAGE.md#6-custom-datasets).\n",
    "\n",
    "For a tutorial on writing a custom cloud/cluster for running jobs, see the [tutorial here](https://github.com/openlema/lema/blob/main/notebooks/LeMa%20-%20Launching%20Jobs%20on%20Custom%20Clusters.ipynb).\n",
    "\n",
    "You can find further information about the required registry decorators [here](https://learning-machines.ai/docs/latest/lema.core.html#lema.core.registry.register_cloud_builder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "Now that you've completed our tour, you're ready to tackle our other [notebook guides](https://github.com/openlema/lema/tree/main/notebooks). \n",
    "\n",
    "Make sure you also take a look at our [Usage guide](https://github.com/openlema/lema/blob/main/USAGE.md) for an overview of our CLI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
