{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This is needed for vLLM to use multiple GPUs in a notebook.\n",
    "# If you're not running in a notebook, you can ignore this.\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from oumi.core.configs import InferenceConfig\n",
    "from oumi.core.types import Conversation, Message, Role\n",
    "from oumi.inference import VLLMInferenceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "Make sure you have the `oumi` package installed: `pip install \".[gpu]\"`\n",
    "\n",
    "If using gated models such `llama3`, make sure to set the `HF_TOKEN` environment variable to your Hugging Face token:\n",
    "\n",
    "```python\n",
    "os.environ[\"HF_TOKEN\"] = \"<my_token>\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have multiple GPUs, we can use Ray to parallelize the inference.\n",
    "# This is essential if you're runnng a model that's too big to fit in a single GPU.\n",
    "\n",
    "import ray\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() >= 2:\n",
    "    ray.shutdown()\n",
    "    ray.init(num_gpus=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the config file\n",
    "\n",
    "Note: in this section we are writing the config file to the current working directory.\n",
    "\n",
    "An alternative option is to initialize the params classes directly: `ModelParams`, `GenerationParams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"llama70b_inference_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama70b_inference_config.yaml\n",
    "\n",
    "model:\n",
    "  # model_name: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # 8B model, requires 1x A100-40GB GPUs\n",
    "  model_name: \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # 70B model, requires 4x A100-40GB GPUs\n",
    "  # model_name: \"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\"  # 4-bit quantized model, requires 1x A100-40GB GPUs. See bonus section for more details.\n",
    "  model_max_length: 512\n",
    "  torch_dtype_str: \"bfloat16\"\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and the inference engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Download, and load the model in memory\n",
    "# This may take a while, depending on your internet speed.\n",
    "# The inference engine only needs to be loaded once and can be\n",
    "# reused for multiple conversations.\n",
    "\n",
    "config = InferenceConfig.from_yaml(config_path)\n",
    "\n",
    "inference_engine = VLLMInferenceEngine(\n",
    "    config.model,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),  # use all available GPUs\n",
    "    # Enable prefix caching for vLLM.\n",
    "    # This is key for performance when running prompts with a long prefix,\n",
    "    # such as judging or conversations with large system prompts\n",
    "    # or few-shot examples.\n",
    "    enable_prefix_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing our inputs\n",
    "\n",
    "The inference engine expects a list of conversations, where each conversation is a list of messages.\n",
    "\n",
    "See the [Conversation](https://github.com/oumi-ai/oumi/blob/38b3d2b27407be5fc9be5a1dd88f9ad518f3491c/src/oumi/core/types/turn.py#L109) class for more details.\n",
    "\n",
    "Tip: you can visualize how the conversation is rendered as a prompt with the following:\n",
    "\n",
    "```python\n",
    "inference_engine.apply_chat_template(conversation, tokenize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = [\n",
    "    Conversation(\n",
    "        messages=[\n",
    "            Message(\n",
    "                role=Role.SYSTEM, content=\"Translate the following text into French.\"\n",
    "            ),\n",
    "            Message(role=Role.USER, content=\"Hello, how are you?\"),\n",
    "        ]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running inference\n",
    "\n",
    "Under the hood, the vLLM engine will batch the conversations to run inference with a high throughput.\n",
    "\n",
    "Make sure to feed all your prompts to the engine at once for maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Running inference for {len(conversations)} conversations\")\n",
    "\n",
    "generations = inference_engine.infer(\n",
    "    input=conversations,\n",
    "    inference_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conversation in generations:\n",
    "    print(repr(conversation))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we also have the following function available:\n",
    "\n",
    "```python\n",
    "inference_engine.infer_from_file(input_filepath=\"path/to/file.json\", inference_config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Running quantized GGUF models\n",
    "\n",
    "You can also run quantized GGUF models, by downloading the model file and passing it to the engine.\n",
    "\n",
    "For example, to run the 70B Meta Llama 3.1 model quantized at 4-bit, you can do the following: \n",
    "\n",
    "First, we download the GGUF model file. There are multiple quantization schemes available, here we choose the `Q4_K_S` scheme which is 4-bit with the `K_S` quantization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"bullerwins/Meta-Llama-3.1-70B-Instruct-GGUF\"\n",
    "filename = \"Meta-Llama-3.1-70B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "# will download the model in the current working directory instead of HF_CACHE_DIR\n",
    "model_path = hf_hub_download(repo_id, filename=filename, local_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update the config file to point to the model we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama70b_inference_config.yaml\n",
    "\n",
    "model:\n",
    "  # Filepath to the GGUF model, which we just downloaded, see `model_path` output above\n",
    "  model_name: \"Meta-Llama-3.1-70B-Instruct-Q4_K_S.gguf\"  \n",
    "  # GGUF files do not have a config. We need to specify the tokenizer name manually.\n",
    "  tokenizer_name: \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  \n",
    "  model_max_length: 512\n",
    "  torch_dtype_str: \"float16\"  # GGUF models require float16\n",
    "  trust_remote_code: True\n",
    "  attn_implementation: \"sdpa\"\n",
    "\n",
    "generation:\n",
    "  max_new_tokens: 128\n",
    "  batch_size: 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
